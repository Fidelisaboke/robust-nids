{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Fidelisaboke/robust-nids/blob/feat%2Fbaseline-model/notebooks/02_sampling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Simplified Sampling for TII-SSRC-23\n",
        "\n",
        "This notebook performs a stratified reservoir sample of the TII-SSRC-23 dataset.\n",
        "It aims to create a balanced 200k-row subset by:\n",
        "1. Keeping ALL benign samples (approx. 1,301).\n",
        "2. Sampling malicious traffic proportionally to the square root of their frequency, ensuring rare attacks are represented.\n",
        "\n"
      ],
      "metadata": {
        "id": "djV8Av87RrU5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installations and Imports"
      ],
      "metadata": {
        "id": "zRiMOHfXSnWH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install fastparquet\n",
        "!pip install pyarrow"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cefg6-HLSmyL",
        "outputId": "89139702-0725-479e-b262-d6bc79973ff0"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting fastparquet\n",
            "  Downloading fastparquet-2024.11.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)\n",
            "Requirement already satisfied: pandas>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from fastparquet) (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from fastparquet) (2.0.2)\n",
            "Requirement already satisfied: cramjam>=2.3 in /usr/local/lib/python3.12/dist-packages (from fastparquet) (2.11.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from fastparquet) (2025.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from fastparquet) (25.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.5.0->fastparquet) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.5.0->fastparquet) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.5.0->fastparquet) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas>=1.5.0->fastparquet) (1.17.0)\n",
            "Downloading fastparquet-2024.11.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.8 MB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.8 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m59.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: fastparquet\n",
            "Successfully installed fastparquet-2024.11.0\n",
            "Requirement already satisfied: pyarrow in /usr/local/lib/python3.12/dist-packages (18.1.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Imports\n",
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pyarrow.dataset as ds\n",
        "from tqdm import tqdm\n",
        "from collections import defaultdict\n",
        "import math"
      ],
      "metadata": {
        "id": "sKs_mA7ARwzc"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Configuration"
      ],
      "metadata": {
        "id": "gMVGsH8rR0c-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Mount Google Drive if needed"
      ],
      "metadata": {
        "id": "Iv9FHv9MSCb_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive if needed (uncomment if running on Colab)\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1yyLg_wpSFkc",
        "outputId": "dae83203-89eb-43ac-e402-940e4a878666"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Paths\n",
        "CSV_PATH = '/content/drive/MyDrive/Datasets/TII-SSRC-23/data.csv'\n",
        "PARQUET_DIR = 'parquet_files'\n",
        "OUT_PATH_CSV = '/content/drive/MyDrive/Datasets/TII-SSRC-23/sampled_200k_simple.csv'\n",
        "OUT_PATH_PARQUET = '/content/drive/MyDrive/Datasets/TII-SSRC-23/sampled_200k_simple.parquet'\n",
        "\n",
        "# Constants\n",
        "N_TOTAL = 200_000\n",
        "CHUNKSIZE = 200_000\n",
        "MIN_PER_SUBTYPE = 50\n",
        "ALPHA = 0.5  # For square-root weighting\n",
        "RANDOM_SEED = 42\n",
        "\n",
        "# Column Definitions\n",
        "TARGET_COL = \"Label\"\n",
        "TYPE_COL = \"Traffic Type\"\n",
        "SUBTYPE_COL = \"Traffic Subtype\"\n",
        "\n",
        "# These are not essential for intrusion detection.\n",
        "COLUMNS_TO_REMOVE = [\n",
        "    'Flow ID',\n",
        "    'Src IP',\n",
        "    'Dst IP',\n",
        "    'Timestamp',\n",
        "]\n",
        "\n",
        "random.seed(RANDOM_SEED)\n",
        "np.random.seed(RANDOM_SEED)"
      ],
      "metadata": {
        "id": "Yyzn41CIR2Pn"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### CSV to Parquet"
      ],
      "metadata": {
        "id": "f1W_h0gGSKr6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.makedirs(PARQUET_DIR, exist_ok=True)\n",
        "for i, chunk in tqdm(enumerate(pd.read_csv(CSV_PATH, chunksize=CHUNKSIZE))):\n",
        "    fname = os.path.join(PARQUET_DIR, f\"part_{i:05d}.parquet\")\n",
        "    chunk.to_parquet(fname, index=False, engine=\"fastparquet\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bqxBuzBiSMf8",
        "outputId": "8ad2c9b2-20b5-4cbf-f9d9-eade76426de5"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "44it [03:20,  4.56s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Helpers"
      ],
      "metadata": {
        "id": "ZyHFFDZNSNbM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================================\n",
        "# 2. Helper Functions\n",
        "# =========================================\n",
        "\n",
        "def get_feature_columns(parquet_dir, remove_cols):\n",
        "    \"\"\"Determines the list of feature columns to keep.\"\"\"\n",
        "    try:\n",
        "        dataset = ds.dataset(parquet_dir, format=\"parquet\")\n",
        "        all_cols = set(dataset.schema.names)\n",
        "        remove_set = set(remove_cols)\n",
        "        # We keep the label columns during sampling for filtering\n",
        "        feature_cols = sorted(list(all_cols - remove_set))\n",
        "        return feature_cols\n",
        "    except Exception as e:\n",
        "        print(f\"Error reading schema: {e}\")\n",
        "        return []\n",
        "\n",
        "def parquet_generator(parquet_dir, chunksize, columns=None):\n",
        "    \"\"\"Yields chunks of data from the parquet dataset.\"\"\"\n",
        "    dataset = ds.dataset(parquet_dir, format=\"parquet\")\n",
        "    scanner = dataset.scanner(batch_size=chunksize, columns=columns)\n",
        "    for batch in scanner.to_batches():\n",
        "        yield batch.to_pandas()"
      ],
      "metadata": {
        "id": "M26df_vCSQoT"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Pass 1: Count Subtypes"
      ],
      "metadata": {
        "id": "E0xkyibXSSD7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================================\n",
        "# 3. Pass 1: Count Subtypes\n",
        "# =========================================\n",
        "print(\"--- Pass 1: Counting Subtypes ---\")\n",
        "\n",
        "subtype_counts = defaultdict(int)\n",
        "total_rows = 0\n",
        "\n",
        "# We only need the Subtype column for counting\n",
        "for chunk in tqdm(parquet_generator(PARQUET_DIR, CHUNKSIZE, columns=[SUBTYPE_COL])):\n",
        "    counts = chunk[SUBTYPE_COL].value_counts()\n",
        "    for subtype, count in counts.items():\n",
        "        subtype_counts[subtype] += count\n",
        "    total_rows += len(chunk)\n",
        "\n",
        "print(f\"\\nTotal rows scanned: {total_rows:,}\")\n",
        "print(\"Subtype counts:\")\n",
        "for s, c in sorted(subtype_counts.items(), key=lambda x: x[1], reverse=True):\n",
        "    print(f\"  {s:<25}: {c:,}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R1IH4eKhSSzR",
        "outputId": "fbdeb84a-37da-4501-da34-f73d40558e57"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Pass 1: Counting Subtypes ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "44it [00:01, 25.55it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Total rows scanned: 8,656,767\n",
            "Subtype counts:\n",
            "  DoS RST                  : 1,072,504\n",
            "  Information Gathering    : 1,038,363\n",
            "  DoS ACK                  : 936,307\n",
            "  DoS PSH                  : 909,507\n",
            "  DoS URG                  : 906,190\n",
            "  DoS CWR                  : 872,523\n",
            "  DoS ECN                  : 871,150\n",
            "  DoS SYN                  : 856,764\n",
            "  DoS FIN                  : 725,600\n",
            "  DoS UDP                  : 257,994\n",
            "  DoS HTTP                 : 82,351\n",
            "  Mirai DDoS DNS           : 55,196\n",
            "  Bruteforce DNS           : 22,179\n",
            "  Mirai DDoS SYN           : 14,210\n",
            "  Mirai DDoS HTTP          : 8,923\n",
            "  Mirai Scan Bruteforce    : 8,731\n",
            "  Bruteforce Telnet        : 4,913\n",
            "  Bruteforce SSH           : 3,967\n",
            "  Mirai DDoS ACK           : 3,779\n",
            "  Bruteforce FTP           : 3,485\n",
            "  Bruteforce HTTP          : 628\n",
            "  Video HTTP               : 376\n",
            "  Video RTP                : 349\n",
            "  Text                     : 209\n",
            "  Audio                    : 190\n",
            "  Video UDP                : 145\n",
            "  Mirai DDoS UDP           : 71\n",
            "  Mirai DDoS GREIP         : 49\n",
            "  Mirai DDoS GREETH        : 43\n",
            "  Background               : 32\n",
            "  DoS MAC                  : 30\n",
            "  DoS ICMP                 : 9\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Allocation Strategy"
      ],
      "metadata": {
        "id": "fPpin7hkSUpD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================================\n",
        "# 4. Allocation Strategy\n",
        "# =========================================\n",
        "print(\"\\n--- Calculating Allocations ---\")\n",
        "\n",
        "# Define Benign Subtypes (to keep 100%)\n",
        "BENIGN_SUBTYPES = [\n",
        "    \"Audio\", \"Background\", \"Video HTTP\", \"Video RTP\", \"Video UDP\", \"Text\"\n",
        "]\n",
        "\n",
        "# 4a. Allocate ALL Benign\n",
        "allocations = {}\n",
        "benign_total = 0\n",
        "for subtype in BENIGN_SUBTYPES:\n",
        "    if subtype in subtype_counts:\n",
        "        count = subtype_counts[subtype]\n",
        "        allocations[subtype] = count\n",
        "        benign_total += count\n",
        "\n",
        "print(f\"Benign samples allocated: {benign_total:,}\")\n",
        "\n",
        "# 4b. Allocate Malicious (Square-Root Weighting)\n",
        "malicious_budget = N_TOTAL - benign_total\n",
        "malicious_subtypes = [s for s in subtype_counts if s not in BENIGN_SUBTYPES]\n",
        "\n",
        "# Calculate weights: weight = count^ALPHA (e.g., sqrt(count))\n",
        "weights = {s: subtype_counts[s]**ALPHA for s in malicious_subtypes}\n",
        "total_weight = sum(weights.values())\n",
        "\n",
        "for s in malicious_subtypes:\n",
        "    # Proportional allocation based on weight\n",
        "    quota = (weights[s] / total_weight) * malicious_budget\n",
        "    # Enforce min/max constraints\n",
        "    quota = max(MIN_PER_SUBTYPE, int(round(quota)))\n",
        "    quota = min(quota, subtype_counts[s])\n",
        "    allocations[s] = quota\n",
        "\n",
        "# 4c. Adjust to exactly match N_TOTAL\n",
        "current_total = sum(allocations.values())\n",
        "delta = N_TOTAL - current_total\n",
        "\n",
        "# Simple adjustment: add/remove from the largest malicious subtypes\n",
        "sorted_malicious = sorted(malicious_subtypes, key=lambda s: allocations[s], reverse=True)\n",
        "idx = 0\n",
        "\n",
        "# Distribute the delta among eligible candidates\n",
        "while delta != 0 and idx < len (sorted_malicious) * 2:\n",
        "    subtype = sorted_malicious[idx % len(sorted_malicious)]\n",
        "    if delta > 0 and allocations[subtype] < subtype_counts[subtype]:\n",
        "        allocations[subtype] += 1\n",
        "        delta -= 1\n",
        "    elif delta < 0 and allocations[subtype] > MIN_PER_SUBTYPE:\n",
        "        allocations[subtype] -= 1\n",
        "        delta += 1\n",
        "    idx += 1\n",
        "\n",
        "# If delta is still not zero, it implies no more adjustments are possible.\n",
        "if delta != 0:\n",
        "  print(f\"Warning: Could not meet exact N_TOTAL. Final delta: {delta}\")\n",
        "\n",
        "print(f\"Total allocated: {sum(allocations.values()):,}\")\n",
        "# print(\"Final Allocations:\", allocations)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e-iWfuogSWKj",
        "outputId": "eb4879c0-9e82-4039-c051-d8c58fbc8b3f"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Calculating Allocations ---\n",
            "Benign samples allocated: 1,301\n",
            "Warning: Could not meet exact N_TOTAL. Final delta: 340\n",
            "Total allocated: 199,660\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Pass 2: Reservoir Sampling"
      ],
      "metadata": {
        "id": "wI5TYR1eSbMb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================================\n",
        "# 5. Pass 2: Reservoir Sampling\n",
        "# =========================================\n",
        "print(\"\\n--- Pass 2: Sampling ---\")\n",
        "\n",
        "# Prepare feature columns (keeping Src/Dst Port!)\n",
        "feature_cols = get_feature_columns(PARQUET_DIR, COLUMNS_TO_REMOVE)\n",
        "print(f\"Features selected: {len(feature_cols)}\")\n",
        "\n",
        "# Initialize reservoirs and seen counters for each subtype\n",
        "reservoirs = {s: [] for s in allocations}\n",
        "seen_counts = {s: 0 for s in allocations}\n",
        "\n",
        "# Main sampling loop\n",
        "# Read all necessary columns now\n",
        "for chunk in tqdm(parquet_generator(PARQUET_DIR, CHUNKSIZE, columns=feature_cols)):\n",
        "    # Optional: Shuffle chunk to randomize input order slightly\n",
        "    # chunk = chunk.sample(frac=1, random_state=RANDOM_SEED).reset_index(drop=True)\n",
        "\n",
        "    # Group by subtype for faster processing than row-by-row\n",
        "    # (This is much faster than iterating every row)\n",
        "    grouped = chunk.groupby(SUBTYPE_COL)\n",
        "\n",
        "    for subtype, group_df in grouped:\n",
        "        if subtype not in allocations: continue\n",
        "\n",
        "        quota = allocations[subtype]\n",
        "        # Convert group to a list of records (dicts) for easier reservoir handling\n",
        "        rows = group_df.to_dict('records')\n",
        "\n",
        "        for row in rows:\n",
        "            seen_counts[subtype] += 1\n",
        "            current_seen = seen_counts[subtype]\n",
        "\n",
        "            # Standard Reservoir Sampling Logic\n",
        "            if len(reservoirs[subtype]) < quota:\n",
        "                # Phase 1: Fill the reservoir\n",
        "                reservoirs[subtype].append(row)\n",
        "            else:\n",
        "                # Phase 2: Randomly replace\n",
        "                # Generate random integer j between 0 and current_seen-1\n",
        "                j = random.randint(0, current_seen - 1)\n",
        "                if j < quota:\n",
        "                    reservoirs[subtype][j] = row"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wdWXKnmUSb96",
        "outputId": "13a0dd8c-078a-4ae9-e2c4-8079ffcbf39d"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Pass 2: Sampling ---\n",
            "Features selected: 82\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "44it [03:33,  4.84s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Finish and Save"
      ],
      "metadata": {
        "id": "r9pDlFL1SfeH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================================\n",
        "# 6. Finalize & Save\n",
        "# =========================================\n",
        "print(\"\\n--- Finalizing and Saving ---\")\n",
        "\n",
        "# Combine all reservoirs into one DataFrame\n",
        "final_samples = []\n",
        "for subtype_samples in reservoirs.values():\n",
        "    final_samples.extend(subtype_samples)\n",
        "\n",
        "df_sampled = pd.DataFrame(final_samples)\n",
        "\n",
        "# Shuffle the final dataset\n",
        "df_sampled = df_sampled.sample(frac=1, random_state=RANDOM_SEED).reset_index(drop=True)\n",
        "\n",
        "print(f\"Final Sample Shape: {df_sampled.shape}\")\n",
        "print(\"Sample Label Distribution:\")\n",
        "print(df_sampled[TARGET_COL].value_counts())\n",
        "\n",
        "# Save to disk\n",
        "df_sampled.to_csv(OUT_PATH_CSV, index=False)\n",
        "print(f\"Saved CSV to: {OUT_PATH_CSV}\")\n",
        "\n",
        "# Optional: Save to parquet for faster loading\n",
        "# df_sampled.to_parquet(OUT_PATH_PARQUET, index=False)\n",
        "# print(f\"Saved Parquet to: {OUT_PATH_PARQUET}\")\n",
        "\n",
        "print(\"\\nDone.\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Finalizing and Saving ---\n",
            "Final Sample Shape: (199660, 82)\n",
            "Sample Label Distribution:\n",
            "Label\n",
            "Malicious    198359\n",
            "Benign         1301\n",
            "Name: count, dtype: int64\n",
            "Saved CSV to: /content/drive/MyDrive/Datasets/TII-SSRC-23/sampled_200k_simple.csv\n",
            "\n",
            "Done.\n"
          ]
        }
      ],
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z22Ep7zfQ0T_",
        "outputId": "05e51c19-e007-451f-b3ed-4342ab076663"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}